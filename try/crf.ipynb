{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, conv_layers, num_labels, batch_size):\n",
    "        \"\"\"\n",
    "        Linear chain CRF as in Assignment 2\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.conv_layers = conv_layers\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        ### Use GPU if available\n",
    "        if self.use_cuda:\n",
    "            [m.cuda() for m in self.modules()]\n",
    "\n",
    "    def init_params(self):\n",
    "        # W and T flattened into one long tensor of shape (4004*1)\n",
    "        self.x = torch.zeros((26*128+26**2,1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Implement the objective of CRF here.\n",
    "        The input (features) to the CRF module should be convolution features.\n",
    "        \"\"\"\n",
    "        features = self.get_conv_feats(X)\n",
    "        W = self.x[:128*26].view(26,128)  # each column of W (128 dim)\n",
    "        T = self.x[128*26:].view(26, 26)\n",
    "        \n",
    "        prediction = crf_decode(W, T, features)\n",
    "        return (prediction)\n",
    "\n",
    "    def loss(self, X, labels):\n",
    "        \"\"\"\n",
    "        Compute the negative conditional log-likelihood of a labelling given a sequence.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # x is a vector. so reshape it into w_y and T\n",
    "        W = self.x[:128*26].view(26,128)  # each column of W (128 dim)\n",
    "        T = self.x[128*26:].view(26, 26)  # T is 26*26\n",
    "        c=1000\n",
    "\n",
    "        features = self.get_conv_feats(X)\n",
    "        \n",
    "        y=torch.zeros(256,64,26)\n",
    "        y[:,:14,:]=labels\n",
    "        # Assuming the feature(ie convoluted input X) shape to be [266*64*128] ie [batchsize*paddedwords*(height*weight)]\n",
    "        # y is reshaped to (256*64*26)\n",
    "\n",
    "        loss = get_crf_obj(features, y, W, T, c)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Return the gradient of the CRF layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        gradient = blah\n",
    "        return gradient\n",
    "\n",
    "    def get_conv_features(self, X):\n",
    "        \"\"\"\n",
    "        Generate convolution features for a given word\n",
    "        \"\"\"\n",
    "        convfeatures = blah\n",
    "        return convfeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crf_obj(X_train, y_train, W, T, c):    \n",
    "    average_log_loss = -c * torch.sum(torch.tensor([compute_log_probability(X, y, W, T) for X, y in zip(X_train, y_train)])) / len(X_train)    \n",
    "    \n",
    "    W_norm = torch.sum(torch.tensor([(torch.norm(Wy.float())) ** 2 for Wy in W])) / 2\n",
    "    T_norm = torch.sum(torch.tensor([torch.sum( torch.tensor([Tij**2 for Tij in row]) )for row in T])) / 2\n",
    "    return average_log_loss + W_norm + T_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probability(X, y, W, T):\n",
    "    m = self.embed_dim # 64\n",
    "    \n",
    "    # X is a tensor of shape (64*128)\n",
    "    # y is a tensor of shape (64*26)\n",
    "    # W is a tensor of shape (26*128)\n",
    "    # T is a tensor of shape (26*26)\n",
    "    \n",
    "    # To convert y from 64*26 to 64*1\n",
    "    \n",
    "    y_nonzero=torch.nonzero(y)[1]\n",
    "    y=torch.nn.ConstantPad1d((0,64-len(v)),-1)(y_nonzero)\n",
    "    \n",
    "    Z = torch.sum(forward_messages(X, W, T)[-1])\n",
    "    \n",
    "    temp=torch.zeros(m)\n",
    "    for s in range(m):\n",
    "        if y[s]<0: break\n",
    "        temp[s]=torch.dot(X[s], W[y[s]])        \n",
    "        \n",
    "    unnormalized = torch.sum(temp)\n",
    "    \n",
    "    temp=torch.zeros(m)\n",
    "    for i in range(len(y) - 1):\n",
    "        if y[i] <0 :break\n",
    "        temp[i]=T[y[i],y[i+1]]\n",
    "        \n",
    "    unnormalized += torch.sum(temp)\n",
    "    \n",
    "    return torch.log(torch.exp(unnormalized.float())/Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_messages(self,X, W, T):\n",
    "    n = self.num_labels      # 26 letters\n",
    "    m = self.embed_dim       # word length\n",
    "    f_msgs = torch.zeros((m, n), dtype=torch.double) \n",
    "    \n",
    "    # Implement the code from both of the comments in section 5.1 of the appendix\n",
    "    for i in range(n):\n",
    "        f_msgs[0, i] = torch.exp(torch.dot(X[0], W[i]).float())\n",
    "    for i in range(1, m):\n",
    "        for a in range(n):\n",
    "            product = torch.exp(torch.dot(X[i], W[a]).float())\n",
    "            tmp = torch.zeros(n)\n",
    "            for b in range(n):\n",
    "                tmp[b] = torch.exp(T[b, a]) * f_msgs[i - 1, b]\n",
    "            f_msgs[i, a] = product * torch.sum(tmp)\n",
    "    return f_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def backward(self):\n",
    "        \"\"\"\n",
    "        Return the gradient of the CRF layer\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        gradient = crf_obj_prime(self.x, X_data, y_data, c)\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_decode(self,W, T, X_train):\n",
    "    y_pred=[]\n",
    "    n= self.num_labels \n",
    "    m=self.embed_dim\n",
    "    \n",
    "    def maxSumBottomUp(X, W, T):\n",
    "        \n",
    "        l = torch.zeros((m, n))\n",
    "        opts = torch.zeros((m, n))\n",
    "        \n",
    "        yStar = torch.zeros(m, dtype=torch.int8)\n",
    "        for i in range(1, m):\n",
    "            for a in range(n):\n",
    "                tmp = torch.zeros(n)\n",
    "                for b in range(n):                    \n",
    "                    tmp[b] = torch.dot(X[i-1], W[b]) + T[a, b] + l[i-1, b]\n",
    "                l[i, a] = max(tmp)\n",
    "        for b in range(n):\n",
    "            opts[m-1, b] = tprch.dot(X[m-1], W[b]) + l[m-1, b]\n",
    "        MAP = max(opts[m-1])\n",
    "        yStar[m-1] = opts[m-1].max(0)[1]\n",
    "        for i in range(m-1, 0, -1):\n",
    "            for b in range(n):\n",
    "                opts[i-1, b] = torch.dot(X[i-1], W[b]) + T[yStar[i], b] + l[i-1, b]\n",
    "            yStar[i-1] = opts[i-1].max(0)[1]\n",
    "\n",
    "        return MAP, yStar    \n",
    "    \n",
    "\n",
    "    for X in X_train:\n",
    "        target=torch.zeros((64,26),dtype=torch.int8)\n",
    "        MAP, yStar=maxSumBottomUp(X, W, T)\n",
    "        for i,j in enumerate(yStar):\n",
    "            target[i][j-1]=1\n",
    "        y_pred.append(target)\n",
    "    return torch.stack(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_obj_prime(self, X_data, y_data, c=1000):\n",
    "    # x is a vector. so reshape it into w_y and T\n",
    "    W = self.x[:128*26].view(26,128)  # each column of W (128 dim)\n",
    "    T = self.x[128*26:].view(26, 26)  # T is 26*26\n",
    "    n= self.num_labels\n",
    "\n",
    "    grad_Ws = torch.zeros((n, 128), dtype=torch.double)\n",
    "    grad_Ts = np.zeros((n, n),  dtype=torch.double)\n",
    "\n",
    "    for X, y in zip(X_data, y_data):\n",
    "        grad_Ws += compute_grad_W(X, y, W, T)\n",
    "        grad_Ts += compute_grad_T(X, y, W, T)\n",
    "    \n",
    "    grad_Ws = (-c) * grad_Ws / float(len(X_data))\n",
    "    grad_Ts = (-c) * grad_Ts / float(len(X_data))\n",
    "\n",
    "    grad_theta = torch.cat([grad_Ws.reshape(-1), grad_Ts.reshape(-1)])\n",
    "    \n",
    "    return grad_theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
