{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still too-good-to-be-true(aka wrong) values, will fix; correct upto score calculation at least\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from data_loader import get_dataset\n",
    "\n",
    "# X_test, y_test = read_data('test.txt')\n",
    "with open('model.txt') as f:\n",
    "    content=f.readlines()\n",
    "x=[float(x.rstrip()) for x in content]\n",
    "\n",
    "\n",
    "num_features = 128 #128\n",
    "num_label = int((math.sqrt(num_features**2+4*len(x)) - num_features)/2) #26\n",
    "W = np.reshape(x[:num_features*num_label], (num_features, num_label), order='F')\n",
    "T = np.reshape(x[num_features*num_label:], (num_label, num_label), order='F')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset()\n",
    "split = int(0.5 * len(dataset.data)) # train-test split\n",
    "train_data, test_data = dataset.data[:split], dataset.data[split:]\n",
    "train_target, test_target = dataset.target[:split], dataset.target[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Messages(x, word_list, word_labels):\n",
    "    num_ex=len(word_list)\n",
    "    w=word_list[0]\n",
    "    num_features = len(w[1]) #128\n",
    "    num_label = int((math.sqrt(num_features**2+4*len(x)) - num_features)/2) #26\n",
    "    W = np.reshape(x[:num_features*num_label], (num_features, num_label), order='F')\n",
    "    T = np.reshape(x[num_features*num_label:], (num_label, num_label), order='F')\n",
    "    l = np.zeros((num_ex,num_label, 100))\n",
    "    r = np.zeros((num_ex,num_label, 100))\n",
    "\n",
    "    for ex in range(num_ex):\n",
    "#         word = word_list[ex]\n",
    "#         word_label = word_labels[ex]\n",
    "#         num_letter = len(word)\n",
    "        word_label = np.nonzero(word_labels[ex])[1]\n",
    "        word = word_list[ex,:len(word_label)]      \n",
    "        num_letter = len(word_label)\n",
    "        \n",
    "\n",
    "        def letter_func(l):\n",
    "#             print(W)\n",
    "#             print(np.matmul(l,W))\n",
    "            return np.matmul(l,W)\n",
    "        \n",
    "        score = np.transpose(np.apply_along_axis(letter_func, 1, word))\n",
    "        \n",
    "\n",
    "        l[ex,:,0] = 0\n",
    "        r[ex,:,num_letter-1] = 0\n",
    "        for i in range(1, num_letter):\n",
    "            v = np.reshape(np.add(l[ex,:,i-1], score[:,i-1]), (num_label,1))\n",
    "#             print(v.shape)\n",
    "            temp = np.add(T, np.tile(v, (1,num_label)))\n",
    "#             print(temp.shape)\n",
    "            max_temp = np.reshape(np.amax(temp, axis=0), (1,num_label))\n",
    "#             print(max_temp.shape)\n",
    "            l[ex,:, i] = np.add(np.log(np.sum(np.exp(np.subtract(temp, np.tile(max_temp, (num_label,1)))), axis=0)), max_temp)\n",
    "            \n",
    "        for i in range(num_letter-2, -1, -1):\n",
    "            v = np.reshape(np.add(r[ex,:,i+1], score[:,i+1]), (num_label,1))  \n",
    "            temp = np.add(T, np.tile(np.transpose(v), (num_label,1)))\n",
    "            max_temp = np.reshape(np.amax(temp, axis=1), (num_label,1))\n",
    "#             print(max_temp.shape)\n",
    "            r[ex,:, i] = np.transpose(np.add(np.log(np.reshape(np.sum(np.exp(np.subtract(temp, np.tile(max_temp, (1,num_label)))), axis=1), (26,1))), max_temp))\n",
    "#             print(r.shape)\n",
    "    return l,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probability(word_list,word_labels,l,r,T,W):\n",
    "    num_ex=len(word_labels)\n",
    "    f = 0\n",
    "    for ex in range(num_ex):\n",
    "#         word = word_list[ex]\n",
    "#         word_label = word_labels[ex]\n",
    "#         num_letter = len(word)\n",
    "        word_label = np.nonzero(word_labels[ex])[1]\n",
    "        word = word_list[ex,:len(word_label)]      \n",
    "        num_letter = len(word_label)\n",
    "\n",
    "\n",
    "        def letter_func(l):\n",
    "#             print(l.shape,W.shape)\n",
    "            return np.matmul(l,W)\n",
    "        score = np.transpose(np.apply_along_axis(letter_func, 1, word))\n",
    "        \n",
    "        \n",
    "        l_plus_score = np.add(l[ex,:, :num_letter], score)\n",
    "        r_plus_score = np.add(r[ex,:, :num_letter], score)     \n",
    "        marg = np.add(l_plus_score, r_plus_score)\n",
    "        marg = np.subtract(marg, score)\n",
    "        t = np.amax(marg[:,0])\n",
    "        f = f - math.log(np.sum(np.exp(marg[:,0]-t))) - t\n",
    "        for i in range(num_letter):\n",
    "                lab = word_label[i]\n",
    "                f = f + score[lab][i]\n",
    "                if i < num_letter-1:\n",
    "                    next_lab = word_label[i+1]\n",
    "                    f = f + T[lab][next_lab]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGrad(word_list,word_labels,T,l,r):\n",
    "    num_ex=len(word_labels)\n",
    "    g_W = np.zeros(W.shape)\n",
    "    g_T = np.zeros(T.shape)\n",
    "    for ex in range(num_ex):\n",
    "#         word = word_list[ex]\n",
    "#         word_label = word_labels[ex]\n",
    "#         num_letter = len(word)\n",
    "        word_label = np.nonzero(word_labels[ex])[1]\n",
    "        word = word_list[ex,:len(word_label)]      \n",
    "        num_letter = len(word_label)\n",
    "        \n",
    "        def letter_func(l):\n",
    "#             print(np.matmul(l,W))\n",
    "            return np.matmul(l,W)\n",
    "        score = np.transpose(np.apply_along_axis(letter_func, 1, word))\n",
    "        \n",
    "        l_plus_score = np.add(l[ex,:, :num_letter], score)\n",
    "        r_plus_score = np.add(r[ex,:, :num_letter], score)     \n",
    "        marg = np.add(l_plus_score, r_plus_score)\n",
    "        marg = np.subtract(marg, score)\n",
    "        marg = np.exp(np.subtract(marg, np.tile(np.amax(marg, axis=0), (num_label, 1))))\n",
    "        marg = np.divide(marg, np.tile(np.sum(marg, axis=0), (num_label, 1)))  \n",
    "        \n",
    "#         print(marg.shape)\n",
    "        for i in range(num_letter):\n",
    "            lab = word_label[i]\n",
    "            V = np.reshape(marg[:,i], (num_label,1))\n",
    "            V[lab] = V[lab] - 1\n",
    "            \n",
    "#             print(V.shape)\n",
    "            g_W = np.subtract(g_W, np.matmul(np.reshape(word[i], (num_features,1)), np.transpose(V)))\n",
    "            if i < num_letter-1:\n",
    "                next_lab = word_label[i+1]\n",
    "                V = np.add(T, np.tile(np.reshape(l_plus_score[:,i], (num_label,1)), (1, num_label)))\n",
    "                V = np.add(V, np.tile(np.transpose(r_plus_score[:,i+1]), (num_label, 1)))\n",
    "                V = np.exp(V - np.amax(V))\n",
    "                g_T = g_T - V / np.sum(V)\n",
    "                g_T[lab][next_lab] = g_T[lab][next_lab] + 1\n",
    "    grad=np.concatenate([g_W.reshape(-1), g_T.reshape(-1)])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-102439.69961427595\n",
      "4.362769603729248\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s=time.time()\n",
    "f,b=Messages(x,train_data, train_target)\n",
    "logp=compute_log_probability(train_data, train_target,f,b,T,W)\n",
    "g=ComputeGrad(train_data, train_target,T,f,b)\n",
    "print(logp)\n",
    "print(time.time()-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logprob,x,C,N):\n",
    "    if C>0:\n",
    "        obj = np.linalg.norm(x)**2/2 - logprob*C/N\n",
    "    else: \n",
    "        print(logprob,N)\n",
    "        obj = logprob / N\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-102439.69961427595 3438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-29.796305879661414"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(logp,x,-1,len(train_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
